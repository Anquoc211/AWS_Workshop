---
title: "Ng√†y 41 - √în t·∫≠p ki·∫øn th·ª©c c∆° b·∫£n NLP"
weight: 1
chapter: false
pre: "<b> 1.9.1. </b>"
---

**Ng√†y:** 2025-11-03 (Th·ª© Hai)  
**Tr·∫°ng th√°i:** "Ho√†n th√†nh"  

---

# **Ghi ch√∫ b√†i gi·∫£ng**

## T√≥m t·∫Øt Tu·∫ßn 8

### C√°c kh√°i ni·ªám NLP c·ªët l√µi ƒë∆∞·ª£c xem l·∫°i

**Quy tr√¨nh ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n**
- Tokenization: c·∫•p ƒë·ªô t·ª´ v√† c√¢u
- Chu·∫©n h√≥a: ch·ªØ th∆∞·ªùng, lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát
- Lo·∫°i b·ªè stop word: quy·∫øt ƒë·ªãnh ph·ª• thu·ªôc ng·ªØ c·∫£nh
- Stemming vs Lemmatization: s·ª± ƒë√°nh ƒë·ªïi v√† tr∆∞·ªùng h·ª£p s·ª≠ d·ª•ng

**Nh·ªØng ƒëi·ªÉm ch√≠nh t·ª´ Tu·∫ßn 8**
- Ch·∫•t l∆∞·ª£ng ti·ªÅn x·ª≠ l√Ω ·∫£nh h∆∞·ªüng tr·ª±c ti·∫øp ƒë·∫øn hi·ªáu su·∫•t m√¥ h√¨nh
- C√°c t√°c v·ª• kh√°c nhau y√™u c·∫ßu chi·∫øn l∆∞·ª£c ti·ªÅn x·ª≠ l√Ω kh√°c nhau
- Lemmatization th∆∞·ªùng ƒë∆∞·ª£c ∆∞u ti√™n cho h·ªá th·ªëng production
- Lu√¥n x√°c th·ª±c l·ª±a ch·ªçn ti·ªÅn x·ª≠ l√Ω v·ªõi tr∆∞·ªùng h·ª£p s·ª≠ d·ª•ng c·ª• th·ªÉ

## ƒêi s√¢u: K·ªπ thu·∫≠t Tokenization

### Tokenization n√¢ng cao

**Subword Tokenization**
- Byte Pair Encoding (BPE): x·ª≠ l√Ω c√°c t·ª´ ngo√†i t·ª´ v·ª±ng
- WordPiece: ƒë∆∞·ª£c BERT v√† c√°c transformer kh√°c s·ª≠ d·ª•ng
- SentencePiece: tokenization kh√¥ng ph·ª• thu·ªôc ng√¥n ng·ªØ
- Tr∆∞·ªùng h·ª£p s·ª≠ d·ª•ng: m√¥ h√¨nh ƒëa ng√¥n ng·ªØ v√† x·ª≠ l√Ω t·ª´ hi·∫øm

**So s√°nh c√°c c√°ch ti·∫øp c·∫≠n**

| Ph∆∞∆°ng ph√°p | ∆Øu ƒëi·ªÉm | Nh∆∞·ª£c ƒëi·ªÉm | T·ªët nh·∫•t cho |
|-------------|---------|------------|--------------|
| Word | ƒê∆°n gi·∫£n, nhanh | T·ª´ v·ª±ng l·ªõn | T√°c v·ª• ƒë∆°n gi·∫£n |
| Character | Kh√¥ng c√≥ v·∫•n ƒë·ªÅ OOV | Chu·ªói d√†i | T√°c v·ª• ch√≠nh t·∫£ |
| Subword | C√°ch ti·∫øp c·∫≠n c√¢n b·∫±ng | Ph·ª©c t·∫°p h∆°n | NLP hi·ªán ƒë·∫°i |

### Tokenization bi·ªÉu th·ª©c ch√≠nh quy

- M·∫´u t√πy ch·ªânh cho vƒÉn b·∫£n ƒë·∫∑c th√π lƒ©nh v·ª±c
- X·ª≠ l√Ω URLs, emails, hashtags
- B·∫£o to√†n d·∫•u c√¢u quan tr·ªçng
- Tokenization vƒÉn b·∫£n y t·∫ø/k·ªπ thu·∫≠t

## Th·ª±c h√†nh t·ªët nh·∫•t v·ªÅ chu·∫©n h√≥a vƒÉn b·∫£n

### Khi n√†o n√™n chu·∫©n h√≥a

**N√™n chu·∫©n h√≥a:**
- ƒê·ªô nh·∫°y ch·ªØ hoa/th∆∞·ªùng kh√¥ng quan tr·ªçng
- C·∫ßn ƒë·ªãnh d·∫°ng nh·∫•t qu√°n
- R√†ng bu·ªôc v·ªÅ b·ªô nh·ªõ/t√≠nh to√°n

**Tr√°nh chu·∫©n h√≥a qu√° m·ª©c:**
- Th·ª±c th·ªÉ c√≥ t√™n quan tr·ªçng
- Ph√¢n t√≠ch c·∫£m x√∫c (bi·ªÉu t∆∞·ª£ng c·∫£m x√∫c quan tr·ªçng)
- Code ho·∫∑c t√†i li·ªáu k·ªπ thu·∫≠t

### X·ª≠ l√Ω Unicode

- M√£ h√≥a/gi·∫£i m√£ ƒë√∫ng c√°ch
- C√°c d·∫°ng chu·∫©n h√≥a (NFC, NFD, NFKC, NFKD)
- X·ª≠ l√Ω vƒÉn b·∫£n ƒëa ng√¥n ng·ªØ
- B·∫£o to√†n emoji v√† k√Ω t·ª± ƒë·∫∑c bi·ªát

## Nh·ªØng hi·ªÉu bi·∫øt quan tr·ªçng

- Xem l·∫°i ki·∫øn th·ª©c c∆° b·∫£n ti·∫øt l·ªô s·ª± hi·ªÉu bi·∫øt s√¢u s·∫Øc h∆°n
- C√°c tr∆∞·ªùng h·ª£p bi√™n th∆∞·ªùng x√°c ƒë·ªãnh chi·∫øn l∆∞·ª£c ti·ªÅn x·ª≠ l√Ω
- NLP hi·ªán ƒë·∫°i ng√†y c√†ng s·ª≠ d·ª•ng subword tokenization
- S·ª± c√¢n b·∫±ng gi·ªØa ƒë∆°n gi·∫£n v√† hi·ªáu qu·∫£ l√† r·∫•t quan tr·ªçng

---

# **Th·ª±c h√†nh Lab**

## Lab 1: So s√°nh ti·ªÅn x·ª≠ l√Ω to√†n di·ªán

```python
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.corpus import stopwords
import string

text = """
The AI-powered system's performance improved significantly! 
Running multiple tests @ 99.9% accuracy. #MachineLearning
"""

def compare_preprocessing(text):
    """So s√°nh c√°c c√°ch ti·∫øp c·∫≠n ti·ªÅn x·ª≠ l√Ω kh√°c nhau"""
    
    # G·ªëc
    print("VƒÇN B·∫¢N G·ªêC:")
    print(text)
    print("\n" + "="*60 + "\n")
    
    # Tokenization c∆° b·∫£n
    words_basic = word_tokenize(text)
    print("TOKENIZATION C∆† B·∫¢N:")
    print(words_basic)
    print(f"S·ªë l∆∞·ª£ng token: {len(words_basic)}\n")
    
    # Ch·ªØ th∆∞·ªùng + lo·∫°i b·ªè d·∫•u c√¢u
    words_clean = [w.lower() for w in words_basic if w not in string.punctuation]
    print("CH·ªÆ TH∆Ø·ªúNG + KH√îNG D·∫§U C√ÇU:")
    print(words_clean)
    print(f"S·ªë l∆∞·ª£ng token: {len(words_clean)}\n")
    
    # Lo·∫°i b·ªè stop word
    stop_words = set(stopwords.words('english'))
    words_no_stop = [w for w in words_clean if w not in stop_words]
    print("KH√îNG STOP WORDS:")
    print(words_no_stop)
    print(f"S·ªë l∆∞·ª£ng token: {len(words_no_stop)}\n")
    
    # Stemming
    stemmer = PorterStemmer()
    words_stemmed = [stemmer.stem(w) for w in words_no_stop]
    print("STEMMED:")
    print(words_stemmed)
    print(f"S·ªë l∆∞·ª£ng token: {len(words_stemmed)}\n")
    
    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    words_lemmatized = [lemmatizer.lemmatize(w, pos='v') for w in words_no_stop]
    print("LEMMATIZED:")
    print(words_lemmatized)
    print(f"S·ªë l∆∞·ª£ng token: {len(words_lemmatized)}\n")

compare_preprocessing(text)
```

## Lab 2: Tokenization t√πy ch·ªânh v·ªõi Regex

```python
import re

def custom_tokenize(text):
    """Tokenization t√πy ch·ªânh b·∫£o to√†n c√°c m·∫´u ƒë·∫∑c bi·ªát"""
    
    # M·∫´u cho c√°c ph·∫ßn t·ª≠ vƒÉn b·∫£n kh√°c nhau
    patterns = [
        r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',  # URLs
        r'[\w.+-]+@[\w-]+\.[\w.-]+',  # Emails
        r'#\w+',  # Hashtags
        r'@\w+',  # Mentions
        r'\d+\.?\d*%',  # Ph·∫ßn trƒÉm
        r'\$\d+\.?\d*',  # Ti·ªÅn
        r'\d{4}-\d{2}-\d{2}',  # Ng√†y th√°ng
        r'\w+',  # T·ª´
        r'[^\w\s]',  # D·∫•u c√¢u
    ]
    
    pattern = '|'.join(patterns)
    tokens = re.findall(pattern, text)
    
    return tokens

# Ki·ªÉm tra tokenization t√πy ch·ªânh
sample = """
Xem https://example.com ƒë·ªÉ bi·∫øt c·∫≠p nh·∫≠t AI!
Li√™n h·ªá: info@ai-company.com
S·ª± ki·ªán: 2025-11-03 @10:00AM
Ng√¢n s√°ch: $50,000 (99.5% t√†i tr·ª£) #TechEvent
"""

tokens = custom_tokenize(sample)
print("Tokenization t√πy ch·ªânh:")
for i, token in enumerate(tokens, 1):
    print(f"{i}. {token}")
```

## Lab 3: X·ª≠ l√Ω vƒÉn b·∫£n ƒëa ng√¥n ng·ªØ

```python
def process_multilingual_text(text):
    """X·ª≠ l√Ω vƒÉn b·∫£n v·ªõi nhi·ªÅu ng√¥n ng·ªØ v√† k√Ω t·ª± ƒë·∫∑c bi·ªát"""
    
    # Chu·∫©n h√≥a Unicode
    import unicodedata
    
    # NFC (Canonical Decomposition, followed by Canonical Composition)
    normalized = unicodedata.normalize('NFC', text)
    
    # X√°c ƒë·ªãnh c√°c m·∫´u ƒë·∫∑c th√π ng√¥n ng·ªØ
    has_emoji = bool(re.search(r'[\U0001F600-\U0001F64F]', text))
    has_cjk = bool(re.search(r'[\u4e00-\u9fff]', text))  # Trung/Nh·∫≠t/H√†n
    has_arabic = bool(re.search(r'[\u0600-\u06ff]', text))
    
    info = {
        'g·ªëc': text,
        'ƒë√£_chu·∫©n_h√≥a': normalized,
        'c√≥_emoji': has_emoji,
        'c√≥_cjk': has_cjk,
        'c√≥_arabic': has_arabic,
        'ƒë·ªô_d√†i': len(text),
        'ƒë·ªô_d√†i_chu·∫©n_h√≥a': len(normalized)
    }
    
    return info

# Ki·ªÉm tra x·ª≠ l√Ω ƒëa ng√¥n ng·ªØ
multilingual_samples = [
    "Hello ‰∏ñÁïå! üåç",
    "Natural Language Processing",
    "caf√© r√©sum√© na√Øve",
    "ŸÖÿ±ÿ≠ÿ®ÿß ÿßŸÑÿπÿßŸÑŸÖ"
]

for sample in multilingual_samples:
    result = process_multilingual_text(sample)
    print(f"\nVƒÉn b·∫£n: {result['g·ªëc']}")
    print(f"ƒê√£ chu·∫©n h√≥a: {result['ƒë√£_chu·∫©n_h√≥a']}")
    print(f"C√≥ Emoji: {result['c√≥_emoji']}")
    print(f"C√≥ CJK: {result['c√≥_cjk']}")
    print(f"C√≥ Arabic: {result['c√≥_arabic']}")
```

## Lab 4: X√¢y d·ª±ng quy tr√¨nh ti·ªÅn x·ª≠ l√Ω

```python
class PreprocessingPipeline:
    """Quy tr√¨nh ti·ªÅn x·ª≠ l√Ω linh ho·∫°t"""
    
    def __init__(self):
        self.steps = []
        self.stemmer = PorterStemmer()
        self.lemmatizer = WordNetLemmatizer()
        self.stop_words = set(stopwords.words('english'))
    
    def add_lowercase(self):
        self.steps.append(('ch·ªØ_th∆∞·ªùng', lambda x: x.lower()))
        return self
    
    def add_tokenization(self):
        self.steps.append(('tokenize', word_tokenize))
        return self
    
    def add_remove_punctuation(self):
        def remove_punct(tokens):
            return [t for t in tokens if t not in string.punctuation]
        self.steps.append(('lo·∫°i_d·∫•u_c√¢u', remove_punct))
        return self
    
    def add_remove_stopwords(self):
        def remove_stop(tokens):
            return [t for t in tokens if t not in self.stop_words]
        self.steps.append(('lo·∫°i_stop_word', remove_stop))
        return self
    
    def add_stemming(self):
        def stem(tokens):
            return [self.stemmer.stem(t) for t in tokens]
        self.steps.append(('stem', stem))
        return self
    
    def add_lemmatization(self, pos='v'):
        def lemmatize(tokens):
            return [self.lemmatizer.lemmatize(t, pos=pos) for t in tokens]
        self.steps.append(('lemmatize', lemmatize))
        return self
    
    def process(self, text):
        result = text
        for step_name, step_func in self.steps:
            result = step_func(result)
            print(f"Sau {step_name}: {result[:100]}...")  # Hi·ªÉn th·ªã 100 k√Ω t·ª± ƒë·∫ßu
        return result

# X√¢y d·ª±ng quy tr√¨nh t√πy ch·ªânh
pipeline = (PreprocessingPipeline()
            .add_lowercase()
            .add_tokenization()
            .add_remove_punctuation()
            .add_remove_stopwords()
            .add_lemmatization())

text = "The students are studying NLP concepts and building amazing applications!"
result = pipeline.process(text)
print(f"\nK·∫øt qu·∫£ cu·ªëi c√πng: {result}")
```

---

# **B√†i t·∫≠p th·ª±c h√†nh**

1. So s√°nh c√°c c√°ch ti·∫øp c·∫≠n ti·ªÅn x·ª≠ l√Ω tr√™n c√°c lo·∫°i vƒÉn b·∫£n kh√°c nhau (tweets, b√†i b√°o, code)
2. X√¢y d·ª±ng quy tr√¨nh ti·ªÅn x·ª≠ l√Ω cho m·ªôt lƒ©nh v·ª±c c·ª• th·ªÉ (y t·∫ø, ph√°p l√Ω, m·∫°ng x√£ h·ªôi)
3. Tri·ªÉn khai tokenization t√πy ch·ªânh ƒë·ªÉ x·ª≠ l√Ω c√°c ƒë·ªãnh d·∫°ng ƒë·∫∑c bi·ªát (m√£ s·∫£n ph·∫©m, IDs)
4. Ph√¢n t√≠ch t√°c ƒë·ªông c·ªßa c√°c l·ª±a ch·ªçn ti·ªÅn x·ª≠ l√Ω kh√°c nhau ƒë·∫øn t√°c v·ª• ph√¢n lo·∫°i
5. T·∫°o benchmark ti·ªÅn x·ª≠ l√Ω so s√°nh s·ª± ƒë√°nh ƒë·ªïi gi·ªØa t·ªëc ƒë·ªô v√† ch·∫•t l∆∞·ª£ng
